---
layout : default
title : "RNN_Train"
tags : Algorithm
---

## RNN Train

---

<div id="index">
<h2>목차</h2>
</div>

---

1. [RNN이란?](#rnn)
2. [Cell과 Output](#celloutput)
3. [Shape](#shape)
4. [세번째 인자](#_3)
4. [두번째 인자](#_2)
4. [첫번째 인자](#_1)
5. [결론](#outro)


<div id="rnn">
<h2>RNN이란?<div style="float:right"><a href="#index">목차</a></div></h2>
</div>

---

본격적인 RNN Training에 대해 알아보겠습니다.

오늘 진행할 학습은

특정 문자가 Input 되었을때

적절한 문자가 Output 되도록 RNN을 학습하겠습니다.

<br>

`hihello`라는 문자열에서

어떤 `h` 뒤에는 `i`가, 다른 `h`뒤에는 `e`가 나오는 등의 학습이 필요합니다.

이런 학습은 `h`앞에 어떤 문자가 있었는지를 알아야 이루어질 수 있습니다.

<div id="celloutput">
<h2>Cell과 Output<div style="float:right"><a href="#index">목차</a></div></h2>
</div>

---


먼저 `hihello`라는 **문자열(Text)**을 구분해보겠습니다.

이 문자열은 `h` `i` `e` `l` `o` 총 5개의 **문자(Vocabulary)**로 이루어집니다.

이 문자는 `h:0` `i:1` `e:2` `l:3` `0:4`처럼 각각의 **Index**를 지정하여 **Dictionary**형으로 구현하겠습니다.

그 후 각각을 **One Hot Encoding** 방식으로 구현하겠습니다.

```
h = [1, 0, 0, 0, 0]
i = [0, 1, 0, 0, 0]
e = [0, 0, 1, 0, 0]
l = [0, 0, 0, 1, 0]
o = [0, 0, 0, 0, 1]
```

Hidden size = 5

Input shape = (1, 6, 5)

Output shape = (1, 6, 5)

로 될 것입니다.

sequence_loss라는 함수가 있습니다.

(예측값, 타겟(y, true data), 가중치(weight))

로 이루어집니다.

[0.3, 0.7]과 [0.1, 0.9]가 있다면

후자가 loss 값이 더 작을 것입니다.


```{python}
#One Hot Encoding
h = [1, 0, 0, 0, 0]
i = [0, 1, 0, 0, 0]
e = [0, 0, 1, 0, 0]
l = [0, 0, 0, 1, 0]
o = [0, 0, 0, 0, 1]

idx2char = ['h', 'i', 'e', 'l', 'o'] #Index 지정

x_data = [[0, 1, 0, 2, 3, 3]] #hihell
x_one_hot = [[h, i, h, e, l, l]]
y_data = [[1, 0, 2, 3, 3, 4]] #iheloo

X = tf.placehloder(tf.float32, [None, sequence_length, input_dim])

Y = tf.placehloder(tf.int32, [None, sequence_length])

cell = tf.contrib.rnn.BasicLSTMCell(num_units = input_dim, state_is_tuple = True)
initial_state = cell.zero_state(batch_size, tf.float32)
outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state, dtype = tf.float32)
weights = tf.ones([batch_sisze, sequence_length])

sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=wegihts) #바로 logits에 outputs을 넣는 것은 좋지않지만, 지금은 단순한 테스트이기에 일단 진행
loss = tf.reduce_mean(sequence_loss)
train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)

prediction - tf.argmax(outputs, axis=2) #0,2,3..

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(2000):
	l, _ = sess.run([loss, train], feed_dict={X:x_one_hot, Y: y_data})
    result = sess.run([prediction], feed_dict={X:x_one_hot})
    print(i, "Loss : ", l, "Prediction :", result, "True Y : ", y_data)
    
    result_str = [idx2char[c] for c in np.squeeze(result)]
    print("\tPredcition str : ", ''.join(result_str))

```





처음에는 다소 난잡하게 진행됩니다.

loss가 점점 감소하며 나중에는 성공적인 문자열을 예측합니다.