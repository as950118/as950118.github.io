---
layout : default
title : "오류 방지"
tags : Algorithm
---

# Algorism-tutorial

---

## 목차

---

1. [Learning rate 조절](#learningrate)
2. [Data Preproccessing](#datapreproccessing)
3. [Overfitting](#ovefitting)


<div id="learningrate">
<h2>Learning rate 조절</h2>
</div>

---

learning rate를 너무 크게 한다면 경사면을 내려가는 속도가 너무 크게 됩니다.

그러면 터무니없이 큰 값이 발생하는 overshooting이 발생합니다.

반대로 너무 작게한다면 속도가 너무 느리므로 비효율 적입니다.

그러므로 적당한 값을 주는 것이 좋습니다.

적당한 값은 딱히 정해져 있진 않습니다.

<div id="datapreproccessing">
<h2>Data Preproccessing</h2>
</div>
---

W(weight)-C(cost) 함수 형태를 W가 2개 있다고 해서 W1-W2 함수 표시하면 점점 작아지는 동그라미가 겹쳐지는 형태로 나타납니다.

(@형태, 혹은 O안에 o가 들어가고 또 그안에 o가 들어가는 모습)

이 형태의 그래프는 O 안을 타고 들어가서 가장 작은 o 에 도착하는 것입니다.

하지만 W 값의 편차가 크다면 찌그러진 형태의 O 가 됩니다.

이러한 경우 오류가 발생하기 쉽습니다.

그래서 Normalization을 해야합니다.

zero-centered 혹은 normalized 방식을 이용합니다.

전자는 중심값을 0으로 하는 것이고

후자는 모든 값을 특정 범위안으로 가두는 것입니다.

후자의 공식은 

```
x' = (x-m) / σ = (x값 - 평균)/표준편차
```

입니다.

파이썬을 이용한다면

```(python)
x_std[:,0] = (x[:,0] - x[:,0].mean()) / x[:,0].std()
```

처럼 할수 있습니다.

<div id="overfitting">
<h2>Over fitting</h2>
</div>
---

학습 데이터에만 유효하게 학습 해버리는 경우입니다.

이 경우에는 다른 값이 들어올 경우 잘못된 결과를 도출해내게 됩니다.

이를 방지하기 위해서는

1. 더 다양한 학습데이터를 제공

2. 중복된 features를 줄이거나

3. regularization 을 해야합니다.

3번을 보면,

Weight가 선의 구부러짐 정도를 결정하게되므로

a * Σ(W^2) 를 추가하여 작아지도록 합니다.

a를 늘이거나 줄이게 되면 Weight의 영향을 크거나 작게 할 수 있습니다.